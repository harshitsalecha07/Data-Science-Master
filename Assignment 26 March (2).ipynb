{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffab46e7-6a05-4321-9ba2-285c5b523ae4",
   "metadata": {},
   "source": [
    "Q1. **Difference between Simple Linear Regression and Multiple Linear Regression:**\n",
    "\n",
    "   - **Simple Linear Regression:** In simple linear regression, there is a single independent variable (predictor variable) used to predict a single dependent variable. The relationship between the variables is modeled as a straight line. For example, predicting a person's weight (dependent variable) based on their height (independent variable).\n",
    "\n",
    "   - **Multiple Linear Regression:** In multiple linear regression, there are two or more independent variables used to predict a single dependent variable. The relationship between the variables is modeled as a linear combination of the independent variables. For example, predicting a house's price (dependent variable) based on its size, number of bedrooms, and location (independent variables).\n",
    "\n",
    "Q2. **Assumptions of Linear Regression and Checking Assumptions:**\n",
    "\n",
    "   Assumptions of linear regression include linearity, independence of errors, homoscedasticity (constant variance of errors), and normally distributed errors. To check these assumptions, you can use diagnostic plots like residual plots, Q-Q plots, and statistical tests such as the Shapiro-Wilk test for normality.\n",
    "\n",
    "Q3. **Interpreting Slope and Intercept in Linear Regression:**\n",
    "\n",
    "   - **Slope (Coefficient):** It represents the change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant. For example, in a salary prediction model, a slope of 5000 for years of experience means that, on average, each additional year of experience is associated with a $5000 increase in salary.\n",
    "\n",
    "   - **Intercept:** It is the predicted value of the dependent variable when all independent variables are set to zero. In most cases, the intercept may not have a meaningful interpretation, especially if setting all variables to zero is not realistic.\n",
    "\n",
    "Q4. **Gradient Descent in Machine Learning:**\n",
    "\n",
    "   Gradient descent is an optimization algorithm used to find the minimum of a function, typically the loss function in machine learning models. It iteratively adjusts model parameters to minimize the loss. It works by computing the gradient (derivative) of the loss with respect to the parameters and updating the parameters in the opposite direction of the gradient.\n",
    "\n",
    "Q5. **Multiple Linear Regression Model:**\n",
    "\n",
    "   Multiple linear regression is a statistical model that predicts a dependent variable using multiple independent variables. The model assumes that the relationship between the variables is linear. It is represented as:\n",
    "\n",
    "   \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon\\]\n",
    "\n",
    "   The key difference from simple linear regression is the inclusion of multiple independent variables (X1, X2, ..., Xn) rather than just one.\n",
    "\n",
    "Q6. **Multicollinearity in Multiple Linear Regression:**\n",
    "\n",
    "   Multicollinearity refers to a situation where two or more independent variables in a multiple linear regression model are highly correlated with each other. It can make it challenging to separate the individual effects of these variables on the dependent variable. To detect multicollinearity, you can use correlation matrices or variance inflation factor (VIF) values. To address it, you can consider removing one of the correlated variables or using techniques like principal component analysis (PCA).\n",
    "\n",
    "Q7. **Polynomial Regression Model:**\n",
    "\n",
    "   Polynomial regression is an extension of linear regression that allows for more complex relationships between the dependent and independent variables. Instead of fitting a straight line, it fits a polynomial curve to the data. The model includes polynomial terms of the independent variable(s) up to a certain degree. For example, a polynomial regression of degree 2 would include terms like \\(X^2\\) in addition to \\(X\\).\n",
    "\n",
    "Q8. **Advantages and Disadvantages of Polynomial Regression:**\n",
    "\n",
    "   - **Advantages:** \n",
    "     - Can capture nonlinear relationships in data.\n",
    "     - Flexible and can fit a wide range of curves.\n",
    "     - Useful when the relationship between variables is not well-described by a linear model.\n",
    "\n",
    "   - **Disadvantages:**\n",
    "     - Prone to overfitting with high-degree polynomials.\n",
    "     - Can be computationally expensive for high degrees.\n",
    "     - Interpretability decreases as complexity increases.\n",
    "\n",
    "   Polynomial regression is preferred when there is evidence of nonlinear relationships in the data but should be used cautiously to avoid overfitting. Linear regression is simpler and more interpretable when the relationship is approximately linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c899666-8ee1-41f5-8998-29090303fbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
