{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66a2436",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator\" regression, is a regularization technique used in linear regression. It differs from other regression techniques, such as ordinary least squares (OLS) regression, in that it adds a penalty term to the OLS objective function. This penalty term is controlled by a hyperparameter called lambda (λ) or alpha (α). The key difference is that Lasso Regression adds a L1 regularization term, which enforces sparsity by driving some feature coefficients to exactly zero. In other words, Lasso can perform feature selection by eliminating less important variables.\n",
    "\n",
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically select a subset of the most relevant features by driving the coefficients of less important features to zero. This can simplify the model, improve interpretability, and potentially enhance predictive performance by reducing overfitting. In contrast, other regression techniques like Ridge Regression do not eliminate features entirely but only shrink their coefficients.\n",
    "\n",
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting them in ordinary linear regression. Each coefficient represents the change in the dependent variable for a one-unit change in the corresponding independent variable, while holding all other variables constant. However, in Lasso, some coefficients may be exactly zero, indicating that the corresponding features have no impact on the prediction.\n",
    "\n",
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "In Lasso Regression, the main tuning parameter is lambda (λ) or alpha (α), which controls the strength of the L1 regularization penalty. Larger values of λ result in more aggressive feature selection and stronger regularization, while smaller values of λ allow more features to have non-zero coefficients. Cross-validation techniques can be used to choose an optimal value of λ that balances model complexity and performance.\n",
    "\n",
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso Regression is primarily designed for linear regression problems, so it may not handle non-linear relationships between independent and dependent variables effectively. However, it can be used in combination with feature engineering techniques, such as creating interaction terms or polynomial features, to capture non-linear relationships to some extent.\n",
    "\n",
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "The main difference between Ridge Regression and Lasso Regression lies in the type of regularization they apply:\n",
    "   - Ridge Regression uses L2 regularization, which adds a penalty term based on the squared magnitude of coefficients.\n",
    "   - Lasso Regression uses L1 regularization, which adds a penalty term based on the absolute magnitude of coefficients.\n",
    "As a result, Ridge tends to shrink all coefficients towards zero but does not typically result in exactly zero coefficients, while Lasso can drive some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features by automatically selecting a subset of features and setting the coefficients of less important features to zero. This helps in reducing the impact of multicollinearity by eliminating redundant variables from the model.\n",
    "\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "Choosing the optimal value of the regularization parameter (lambda or alpha) in Lasso Regression is typically done through cross-validation techniques. You can try different values of λ and evaluate the model's performance using techniques like k-fold cross-validation or leave-one-out cross-validation. The λ value that minimizes a chosen performance metric (e.g., mean squared error) on the validation dataset is usually selected as the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca154a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
